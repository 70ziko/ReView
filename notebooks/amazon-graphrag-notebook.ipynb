{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews GraphRAG Implementation\n",
    "\n",
    "This notebook implements a GraphRAG system for Amazon product reviews using ArangoDB\n",
    "for graph storage and querying, and LangChain/LangGraph for the agentic components.\n",
    "\n",
    "# Step 0: Package Installation & Setup\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nx-arangodb langchain langchain-community langchain-openai langgraph\n",
    "%pip install pandas numpy matplotlib tqdm openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import nx_arangodb as nxadb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from arango import ArangoClient\n",
    "\n",
    "# For the agentic components\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the ArangoDB database\n",
    "# Replace with your own credentials\n",
    "client = ArangoClient(hosts=\"http://localhost:8529\")\n",
    "db = client.db(\"_system\", username=\"root\", password=\"yourpassword\", verify=True)\n",
    "\n",
    "print(f\"Connected to ArangoDB: {client.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load and Transform the Amazon Review Data\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, target_path):\n",
    "    \"\"\"Download a file from URL to target path with progress bar\"\"\"\n",
    "    os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"File already exists: {target_path}\")\n",
    "        return target_path\n",
    "    \n",
    "    print(f\"Downloading {url} to {target_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Set up progress bar\n",
    "        response = urllib.request.urlopen(url)\n",
    "        file_size = int(response.headers.get('Content-Length', 0))\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(total=file_size, unit='B', unit_scale=True, desc=os.path.basename(target_path))\n",
    "        \n",
    "        # Download with progress updates\n",
    "        with open(target_path, 'wb') as f:\n",
    "            block_size = 8192\n",
    "            while True:\n",
    "                buffer = response.read(block_size)\n",
    "                if not buffer:\n",
    "                    break\n",
    "                f.write(buffer)\n",
    "                progress.update(len(buffer))\n",
    "                \n",
    "        progress.close()\n",
    "        print(f\"Downloaded {target_path}\")\n",
    "        return target_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_gzip(gzip_path, extract_path=None):\n",
    "    \"\"\"Extract a gzip file to the specified path\"\"\"\n",
    "    if extract_path is None:\n",
    "        # Remove .gz extension if present\n",
    "        extract_path = gzip_path[:-3] if gzip_path.endswith('.gz') else gzip_path + '_extracted'\n",
    "    \n",
    "    if os.path.exists(extract_path):\n",
    "        print(f\"Extracted file already exists: {extract_path}\")\n",
    "        return extract_path\n",
    "    \n",
    "    print(f\"Extracting {gzip_path} to {extract_path}\")\n",
    "    try:\n",
    "        with gzip.open(gzip_path, 'rb') as f_in:\n",
    "            with open(extract_path, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        print(f\"Extracted to {extract_path}\")\n",
    "        return extract_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {gzip_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_json_data(file_path, limit=None):\n",
    "    \"\"\"Load JSON data from file, line by line\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON on line {i+1}\")\n",
    "    return data\n",
    "\n",
    "def parse_download_links(file_path):\n",
    "    \"\"\"Parse download links from file\"\"\"\n",
    "    links = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(': ')\n",
    "            if len(parts) == 2:\n",
    "                category, url = parts\n",
    "                links[category] = url\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directories\n",
    "DATA_DIR = \"amazon_data\"\n",
    "DOWNLOAD_DIR = os.path.join(DATA_DIR, \"downloaded\")\n",
    "EXTRACTED_DIR = os.path.join(DATA_DIR, \"extracted\")\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACTED_DIR, exist_ok=True)\n",
    "\n",
    "# Parse download links\n",
    "download_links = parse_download_links(\"paste.txt\")\n",
    "print(f\"Found {len(download_links)} category download links\")\n",
    "\n",
    "# Metadata URL pattern\n",
    "# Assuming metadata URLs follow the same pattern with 'meta_' prefix\n",
    "META_URL_BASE = \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process a single category\n",
    "def process_category(category, review_url, sample_size=1000, meta_sample_size=500):\n",
    "    \"\"\"Download, extract, and load data for a single category\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing category: {category}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Determine file paths\n",
    "    review_gz_path = os.path.join(DOWNLOAD_DIR, f\"{category}.jsonl.gz\")\n",
    "    review_path = os.path.join(EXTRACTED_DIR, f\"{category}.jsonl\")\n",
    "    \n",
    "    # For metadata, construct the URL and paths\n",
    "    meta_url = META_URL_BASE + category + \".jsonl.gz\"\n",
    "    meta_gz_path = os.path.join(DOWNLOAD_DIR, f\"meta_{category}.jsonl.gz\")\n",
    "    meta_path = os.path.join(EXTRACTED_DIR, f\"meta_{category}.jsonl\")\n",
    "    \n",
    "    # Download and extract review data\n",
    "    download_file(review_url, review_gz_path)\n",
    "    extract_gzip(review_gz_path, review_path)\n",
    "    \n",
    "    # Download and extract metadata\n",
    "    try:\n",
    "        download_file(meta_url, meta_gz_path)\n",
    "        extract_gzip(meta_gz_path, meta_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading metadata for {category}: {e}\")\n",
    "        print(\"Continuing without metadata for this category\")\n",
    "        meta_path = None\n",
    "    \n",
    "    # Load review data (with limit for testing)\n",
    "    print(f\"Loading review data for {category}...\")\n",
    "    reviews_data = load_json_data(review_path, limit=sample_size)\n",
    "    print(f\"Loaded {len(reviews_data)} reviews\")\n",
    "    \n",
    "    # Load metadata if available\n",
    "    metadata_data = []\n",
    "    if meta_path and os.path.exists(meta_path):\n",
    "        print(f\"Loading product metadata for {category}...\")\n",
    "        metadata_data = load_json_data(meta_path, limit=meta_sample_size)\n",
    "        print(f\"Loaded {len(metadata_data)} product metadata entries\")\n",
    "    \n",
    "    return reviews_data, metadata_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process categories\n",
    "For testing, we'll just process one category first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify this to process all categories or a subset\n",
    "CATEGORIES_TO_PROCESS = list(download_links.keys())[:1]  # Start with just one for testing\n",
    "print(f\"Will process {len(CATEGORIES_TO_PROCESS)} categories: {CATEGORIES_TO_PROCESS}\")\n",
    "\n",
    "# For demonstration, process just the first category\n",
    "CATEGORY = CATEGORIES_TO_PROCESS[0]\n",
    "reviews_data, metadata_data = process_category(CATEGORY, download_links[CATEGORY])\n",
    "\n",
    "# You can expand this to process all categories sequentially or in parallel\n",
    "# Example for processing all categories:\n",
    "\"\"\"\n",
    "all_reviews = {}\n",
    "all_metadata = {}\n",
    "for category, url in download_links.items():\n",
    "    reviews, metadata = process_category(category, url)\n",
    "    all_reviews[category] = reviews\n",
    "    all_metadata[category] = metadata\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Create and Configure the ArangoDB Graph\n",
    "-----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the collections needed for our graph\n",
    "GRAPH_NAME = \"AmazonReviews\"\n",
    "COLLECTIONS = {\n",
    "    \"nodes\": [\"Products\", \"Reviews\", \"Users\", \"Categories\"],\n",
    "    \"edges\": [\"HasReview\", \"WrittenBy\", \"BelongsToCategory\", \"VariantOf\"]\n",
    "}\n",
    "\n",
    "# Create collections if they don't exist\n",
    "def ensure_collections_exist():\n",
    "    \"\"\"Create the required collections if they don't exist\"\"\"\n",
    "    # Create node collections\n",
    "    for collection in COLLECTIONS[\"nodes\"]:\n",
    "        if not db.has_collection(collection):\n",
    "            db.create_collection(collection)\n",
    "            print(f\"Created collection: {collection}\")\n",
    "            \n",
    "            # Create indexes for faster queries\n",
    "            if collection == \"Products\":\n",
    "                # Index on ASIN (product ID)\n",
    "                db.collection(collection).add_hash_index([\"_key\"], unique=True)\n",
    "                # Index on main category for category-based queries\n",
    "                db.collection(collection).add_hash_index([\"main_category\"], unique=False)\n",
    "                # Index on product parent ASIN for variant relationships\n",
    "                db.collection(collection).add_hash_index([\"parent_asin\"], unique=False)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "                \n",
    "            elif collection == \"Reviews\":\n",
    "                # Index on product ASIN for finding reviews of a product\n",
    "                db.collection(collection).add_hash_index([\"asin\"], unique=False)\n",
    "                # Index on user ID for finding reviews by a user\n",
    "                db.collection(collection).add_hash_index([\"user_id\"], unique=False)\n",
    "                # Index on timestamp for chronological queries\n",
    "                db.collection(collection).add_skiplist_index([\"timestamp\"], unique=False)\n",
    "                # Index on rating for filtering by rating\n",
    "                db.collection(collection).add_skiplist_index([\"rating\"], unique=False)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "                \n",
    "            elif collection == \"Users\":\n",
    "                # Index on user ID\n",
    "                db.collection(collection).add_hash_index([\"_key\"], unique=True)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "                \n",
    "            elif collection == \"Categories\":\n",
    "                # Index on category name\n",
    "                db.collection(collection).add_hash_index([\"_key\"], unique=True)\n",
    "                # Index on category level for hierarchical queries\n",
    "                db.collection(collection).add_skiplist_index([\"level\"], unique=False)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "        else:\n",
    "            print(f\"Collection already exists: {collection}\")\n",
    "    \n",
    "    # Create edge collections\n",
    "    for collection in COLLECTIONS[\"edges\"]:\n",
    "        if not db.has_collection(collection):\n",
    "            db.create_collection(collection, edge=True)\n",
    "            print(f\"Created edge collection: {collection}\")\n",
    "        else:\n",
    "            print(f\"Edge collection already exists: {collection}\")\n",
    "\n",
    "# Create the graph if it doesn't exist\n",
    "def ensure_graph_exists():\n",
    "    \"\"\"Create the graph if it doesn't exist\"\"\"\n",
    "    if not db.has_graph(GRAPH_NAME):\n",
    "        graph = db.create_graph(GRAPH_NAME)\n",
    "        \n",
    "        # Define edge definitions\n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"HasReview\",\n",
    "            from_vertex_collections=[\"Products\"],\n",
    "            to_vertex_collections=[\"Reviews\"]\n",
    "        )\n",
    "        \n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"WrittenBy\",\n",
    "            from_vertex_collections=[\"Reviews\"],\n",
    "            to_vertex_collections=[\"Users\"]\n",
    "        )\n",
    "        \n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"BelongsToCategory\",\n",
    "            from_vertex_collections=[\"Products\"],\n",
    "            to_vertex_collections=[\"Categories\"]\n",
    "        )\n",
    "        \n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"VariantOf\",\n",
    "            from_vertex_collections=[\"Products\"],\n",
    "            to_vertex_collections=[\"Products\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"Created graph: {GRAPH_NAME}\")\n",
    "    else:\n",
    "        print(f\"Graph already exists: {GRAPH_NAME}\")\n",
    "\n",
    "# Create collections and graph\n",
    "ensure_collections_exist()\n",
    "ensure_graph_exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Transform and Insert the Data\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metadata_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m edges\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Transform the data\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m products \u001b[38;5;241m=\u001b[39m prepare_products_data(\u001b[43mmetadata_data\u001b[49m)\n\u001b[1;32m    155\u001b[0m reviews \u001b[38;5;241m=\u001b[39m prepare_reviews_data(reviews_data)\n\u001b[1;32m    156\u001b[0m users \u001b[38;5;241m=\u001b[39m extract_users(reviews)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare products data\n",
    "def prepare_products_data(metadata_list):\n",
    "    \"\"\"Transform metadata into a format suitable for Products collection\"\"\"\n",
    "    products = []\n",
    "    for item in metadata_list:\n",
    "        # Combine description into a single string if it's a list\n",
    "        description = ' '.join(item.get('description', [])) if isinstance(item.get('description', []), list) else item.get('description', '')\n",
    "        \n",
    "        # Extract features as a list\n",
    "        features = item.get('features', [])\n",
    "        features_text = ' '.join(features) if features else ''\n",
    "        \n",
    "        # Prepare product document\n",
    "        product = {\n",
    "            \"_key\": item.get('asin', ''),\n",
    "            \"title\": item.get('title', ''),\n",
    "            \"main_category\": item.get('main_category', ''),\n",
    "            \"description\": description,\n",
    "            \"features\": features,\n",
    "            \"features_text\": features_text,\n",
    "            \"price\": item.get('price', 0),\n",
    "            \"average_rating\": item.get('average_rating', 0),\n",
    "            \"rating_count\": item.get('rating_number', 0),\n",
    "            \"store\": item.get('store', ''),\n",
    "            \"details\": item.get('details', {}),\n",
    "            \"parent_asin\": item.get('parent_asin', ''),\n",
    "            \"bought_together\": item.get('bought_together', []),\n",
    "            \"images\": [img.get('large', '') for img in item.get('images', []) if 'large' in img][:3]  # Store up to 3 image URLs\n",
    "        }\n",
    "        products.append(product)\n",
    "    return products\n",
    "\n",
    "# Prepare reviews data\n",
    "def prepare_reviews_data(reviews_list):\n",
    "    \"\"\"Transform reviews into a format suitable for Reviews collection\"\"\"\n",
    "    reviews = []\n",
    "    for item in reviews_list:\n",
    "        # Create a unique key for the review using asin + user_id + timestamp\n",
    "        timestamp = item.get('sort_timestamp', '')\n",
    "        key = f\"{item.get('asin', '')}-{item.get('user_id', '')}-{timestamp}\"\n",
    "        \n",
    "        # Extract image URLs if available\n",
    "        images = []\n",
    "        for img in item.get('images', []):\n",
    "            if 'large_image_url' in img:\n",
    "                images.append(img['large_image_url'])\n",
    "        \n",
    "        # Prepare review document\n",
    "        review = {\n",
    "            \"_key\": key,\n",
    "            \"asin\": item.get('asin', ''),\n",
    "            \"user_id\": item.get('user_id', ''),\n",
    "            \"parent_asin\": item.get('parent_asin', ''),\n",
    "            \"rating\": item.get('rating', 0),\n",
    "            \"title\": item.get('title', ''),\n",
    "            \"text\": item.get('text', ''),\n",
    "            \"timestamp\": timestamp,\n",
    "            \"helpful_votes\": item.get('helpful_votes', 0),\n",
    "            \"verified_purchase\": item.get('verified_purchase', False),\n",
    "            \"images\": images\n",
    "        }\n",
    "        reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "# Extract users from reviews\n",
    "def extract_users(reviews_list):\n",
    "    \"\"\"Extract unique users from reviews data\"\"\"\n",
    "    users = {}\n",
    "    for review in reviews_list:\n",
    "        user_id = review.get('user_id', '')\n",
    "        if user_id and user_id not in users:\n",
    "            users[user_id] = {\n",
    "                \"_key\": user_id,\n",
    "                \"review_count\": 1\n",
    "            }\n",
    "        elif user_id:\n",
    "            users[user_id][\"review_count\"] += 1\n",
    "    return list(users.values())\n",
    "\n",
    "# Extract categories from products\n",
    "def extract_categories(products_list):\n",
    "    \"\"\"Extract unique categories from product metadata\"\"\"\n",
    "    categories = {}\n",
    "    for product in products_list:\n",
    "        main_category = product.get('main_category', '')\n",
    "        if main_category and main_category not in categories:\n",
    "            categories[main_category] = {\n",
    "                \"_key\": main_category.replace(' ', '_'),\n",
    "                \"name\": main_category,\n",
    "                \"level\": 0  # Top level category\n",
    "            }\n",
    "        \n",
    "        # Process hierarchical categories if available\n",
    "        if 'categories' in product and isinstance(product['categories'], list):\n",
    "            for i, category_list in enumerate(product['categories']):\n",
    "                if isinstance(category_list, list):\n",
    "                    for j, category in enumerate(category_list):\n",
    "                        if category and category not in categories:\n",
    "                            categories[category] = {\n",
    "                                \"_key\": category.replace(' ', '_'),\n",
    "                                \"name\": category,\n",
    "                                \"level\": j+1  # Level in hierarchy\n",
    "                            }\n",
    "    return list(categories.values())\n",
    "\n",
    "# Create edge data\n",
    "def create_has_review_edges(reviews):\n",
    "    \"\"\"Create edges connecting products to reviews\"\"\"\n",
    "    edges = []\n",
    "    for review in reviews:\n",
    "        edge = {\n",
    "            \"_from\": f\"Products/{review['asin']}\",\n",
    "            \"_to\": f\"Reviews/{review['_key']}\"\n",
    "        }\n",
    "        edges.append(edge)\n",
    "    return edges\n",
    "\n",
    "def create_written_by_edges(reviews):\n",
    "    \"\"\"Create edges connecting reviews to users\"\"\"\n",
    "    edges = []\n",
    "    for review in reviews:\n",
    "        edge = {\n",
    "            \"_from\": f\"Reviews/{review['_key']}\",\n",
    "            \"_to\": f\"Users/{review['user_id']}\"\n",
    "        }\n",
    "        edges.append(edge)\n",
    "    return edges\n",
    "\n",
    "def create_belongs_to_category_edges(products):\n",
    "    \"\"\"Create edges connecting products to categories\"\"\"\n",
    "    edges = []\n",
    "    for product in products:\n",
    "        if product.get('main_category'):\n",
    "            edge = {\n",
    "                \"_from\": f\"Products/{product['_key']}\",\n",
    "                \"_to\": f\"Categories/{product['main_category'].replace(' ', '_')}\"\n",
    "            }\n",
    "            edges.append(edge)\n",
    "    return edges\n",
    "\n",
    "def create_variant_of_edges(products):\n",
    "    \"\"\"Create edges connecting product variants\"\"\"\n",
    "    edges = []\n",
    "    for product in products:\n",
    "        if product.get('parent_asin') and product.get('parent_asin') != product.get('_key'):\n",
    "            edge = {\n",
    "                \"_from\": f\"Products/{product['_key']}\",\n",
    "                \"_to\": f\"Products/{product['parent_asin']}\"\n",
    "            }\n",
    "            edges.append(edge)\n",
    "    return edges\n",
    "\n",
    "# Transform the data\n",
    "products = prepare_products_data(metadata_data)\n",
    "reviews = prepare_reviews_data(reviews_data)\n",
    "users = extract_users(reviews)\n",
    "categories = extract_categories(products)\n",
    "\n",
    "# Create edge data\n",
    "has_review_edges = create_has_review_edges(reviews)\n",
    "written_by_edges = create_written_by_edges(reviews)\n",
    "belongs_to_category_edges = create_belongs_to_category_edges(products)\n",
    "variant_of_edges = create_variant_of_edges(products)\n",
    "\n",
    "# Insert data into ArangoDB collections\n",
    "def insert_documents(collection_name, documents, batch_size=100):\n",
    "    \"\"\"Insert documents into a collection with batch processing\"\"\"\n",
    "    collection = db.collection(collection_name)\n",
    "    \n",
    "    # Process in batches to avoid memory issues with large datasets\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        try:\n",
    "            collection.import_bulk(batch, on_duplicate=\"update\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting batch into {collection_name}: {e}\")\n",
    "    \n",
    "    print(f\"Inserted {len(documents)} documents into {collection_name}\")\n",
    "\n",
    "# Insert node data\n",
    "print(\"Inserting product data...\")\n",
    "insert_documents(\"Products\", products)\n",
    "\n",
    "print(\"Inserting review data...\")\n",
    "insert_documents(\"Reviews\", reviews)\n",
    "\n",
    "print(\"Inserting user data...\")\n",
    "insert_documents(\"Users\", users)\n",
    "\n",
    "print(\"Inserting category data...\")\n",
    "insert_documents(\"Categories\", categories)\n",
    "\n",
    "# Insert edge data\n",
    "print(\"Creating product-review connections...\")\n",
    "insert_documents(\"HasReview\", has_review_edges)\n",
    "\n",
    "print(\"Creating review-user connections...\")\n",
    "insert_documents(\"WrittenBy\", written_by_edges)\n",
    "\n",
    "print(\"Creating product-category connections...\")\n",
    "insert_documents(\"BelongsToCategory\", belongs_to_category_edges)\n",
    "\n",
    "print(\"Creating product variant connections...\")\n",
    "insert_documents(\"VariantOf\", variant_of_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create the NetworkX Integration (for Graph Analytics)\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the NetworkX graph from ArangoDB\n",
    "G_adb = nxadb.Graph(name=GRAPH_NAME, db=db)\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Generate and Store Embeddings\n",
    "------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-your-openai-api-key\"\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create embeddings for products\n",
    "def generate_product_embeddings():\n",
    "    \"\"\"Generate embeddings for product data and store in ArangoDB\"\"\"\n",
    "    products_collection = db.collection(\"Products\")\n",
    "    batch_size = 25  # Process in small batches to avoid rate limits\n",
    "    \n",
    "    for i in range(0, products_collection.count(), batch_size):\n",
    "        # Get a batch of products\n",
    "        products_batch = list(products_collection.all().limit(batch_size).offset(i))\n",
    "        \n",
    "        # Prepare text for embedding\n",
    "        product_texts = []\n",
    "        for product in products_batch:\n",
    "            # Combine title, features, and description for a rich representation\n",
    "            text = f\"{product.get('title', '')} {product.get('features_text', '')} {product.get('description', '')}\"\n",
    "            product_texts.append(text)\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        print(f\"Generating embeddings for products {i} to {i+len(product_texts)}\")\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(product_texts)\n",
    "            \n",
    "            # Update products with embeddings\n",
    "            for j, product in enumerate(products_batch):\n",
    "                products_collection.update({\n",
    "                    \"_key\": product[\"_key\"],\n",
    "                    \"embedding\": embeddings[j]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "\n",
    "# Create embeddings for reviews\n",
    "def generate_review_embeddings():\n",
    "    \"\"\"Generate embeddings for review data and store in ArangoDB\"\"\"\n",
    "    reviews_collection = db.collection(\"Reviews\")\n",
    "    batch_size = 25  # Process in small batches to avoid rate limits\n",
    "    \n",
    "    for i in range(0, reviews_collection.count(), batch_size):\n",
    "        # Get a batch of reviews\n",
    "        reviews_batch = list(reviews_collection.all().limit(batch_size).offset(i))\n",
    "        \n",
    "        # Prepare text for embedding\n",
    "        review_texts = []\n",
    "        for review in reviews_batch:\n",
    "            # Combine title and text for a rich representation\n",
    "            text = f\"{review.get('title', '')} {review.get('text', '')}\"\n",
    "            review_texts.append(text)\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        print(f\"Generating embeddings for reviews {i} to {i+len(review_texts)}\")\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(review_texts)\n",
    "            \n",
    "            # Update reviews with embeddings\n",
    "            for j, review in enumerate(reviews_batch):\n",
    "                reviews_collection.update({\n",
    "                    \"_key\": review[\"_key\"],\n",
    "                    \"embedding\": embeddings[j]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "\n",
    "# Generate embeddings (uncomment to execute - can take time for large datasets)\n",
    "# generate_product_embeddings()\n",
    "# generate_review_embeddings()\n",
    "print(\"Embeddings generation code is ready but commented out to avoid API costs. Uncomment as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Build the Agentic App with LangChain & LangGraph\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4o\")\n",
    "\n",
    "# Create the ArangoGraph LangChain wrapper\n",
    "arango_graph = ArangoGraph(db)\n",
    "\n",
    "# Define tools for the agent\n",
    "\n",
    "@tool\n",
    "def get_product_by_description(query: str):\n",
    "    \"\"\"\n",
    "    This tool finds products that match a given description.\n",
    "    It searches through product titles, features, and descriptions to find the best matches.\n",
    "    \"\"\"\n",
    "    # Convert query to embedding\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    \n",
    "    # Prepare AQL query with vector search\n",
    "    aql = \"\"\"\n",
    "    FOR product IN Products\n",
    "        LET score = COSINE_SIMILARITY(product.embedding, @embedding)\n",
    "        FILTER score > 0.7\n",
    "        SORT score DESC\n",
    "        LIMIT 5\n",
    "        RETURN {\n",
    "            asin: product._key,\n",
    "            title: product.title,\n",
    "            description: product.description,\n",
    "            price: product.price,\n",
    "            average_rating: product.average_rating,\n",
    "            score: score\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query\n",
    "    cursor = db.aql.execute(aql, bind_vars={\"embedding\": query_embedding})\n",
    "    results = list(cursor)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No products found matching your description.\"\n",
    "    \n",
    "    # Format results\n",
    "    response = \"Here are products that match your description:\\n\\n\"\n",
    "    for i, product in enumerate(results, 1):\n",
    "        response += f\"{i}. {product['title']}\\n\"\n",
    "        response += f\"   Price: ${product['price']}\\n\"\n",
    "        response += f\"   Rating: {product['average_rating']}/5.0\\n\"\n",
    "        response += f\"   ASIN: {product['asin']}\\n\\n\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def get_reviews_for_product(asin: str):\n",
    "    \"\"\"\n",
    "    This tool retrieves reviews for a specific product identified by its ASIN.\n",
    "    It returns the most helpful reviews first.\n",
    "    \"\"\"\n",
    "    # Prepare AQL query\n",
    "    aql = \"\"\"\n",
    "    FOR review IN Reviews\n",
    "        FILTER review.asin == @asin\n",
    "        SORT review.helpful_votes DESC\n",
    "        LIMIT 5\n",
    "        RETURN {\n",
    "            title: review.title,\n",
    "            text: review.text,\n",
    "            rating: review.rating,\n",
    "            helpful_votes: review.helpful_votes,\n",
    "            verified_purchase: review.verified_purchase\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query\n",
    "    cursor = db.aql.execute(aql, bind_vars={\"asin\": asin})\n",
    "    reviews = list(cursor)\n",
    "    \n",
    "    if not reviews:\n",
    "        return f\"No reviews found for product with ASIN {asin}.\"\n",
    "    \n",
    "    # Get product title\n",
    "    product_query = \"\"\"\n",
    "    FOR product IN Products\n",
    "        FILTER product._key == @asin\n",
    "        RETURN product.title\n",
    "    \"\"\"\n",
    "    product_cursor = db.aql.execute(product_query, bind_vars={\"asin\": asin})\n",
    "    product_title = list(product_cursor)[0] if list(product_cursor) else \"Unknown Product\"\n",
    "    \n",
    "    # Format results\n",
    "    response = f\"Reviews for {product_title} (ASIN: {asin}):\\n\\n\"\n",
    "    for i, review in enumerate(reviews, 1):\n",
    "        response += f\"{i}. {review['title']} - {review['rating']}/5.0 stars\\n\"\n",
    "        response += f\"   {review['text']}\\n\"\n",
    "        if review['verified_purchase']:\n",
    "            response += f\"   (Verified Purchase)\\n\"\n",
    "        response += f\"   Helpful votes: {review['helpful_votes']}\\n\\n\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def text_to_aql_to_text(query: str):\n",
    "    \"\"\"\n",
    "    This tool translates a natural language query into AQL, executes it,\n",
    "    and returns the results in natural language.\n",
    "    Use this for complex searches and relationships in the product graph.\n",
    "    \"\"\"\n",
    "    chain = ArangoGraphQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=arango_graph,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    result = chain.invoke(query)\n",
    "    return str(result[\"result\"])\n",
    "\n",
    "@tool\n",
    "def analyze_product_network(query: str):\n",
    "    \"\"\"\n",
    "    This tool uses graph analytics to analyze the product network.\n",
    "    It can identify related products, popular items, and customer patterns.\n",
    "    \"\"\"\n",
    "    # Parse the query to determine what kind of analysis to perform\n",
    "    if \"popular\" in query.lower() or \"best selling\" in query.lower():\n",
    "        # Find most reviewed products\n",
    "        aql = \"\"\"\n",
    "        FOR product IN Products\n",
    "            SORT product.rating_count DESC\n",
    "            LIMIT 5\n",
    "            RETURN {\n",
    "                asin: product._key,\n",
    "                title: product.title,\n",
    "                reviews: product.rating_count,\n",
    "                rating: product.average_rating\n",
    "            }\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        products = list(cursor)\n",
    "        \n",
    "        response = \"Most popular products based on number of reviews:\\n\\n\"\n",
    "        for i, product in enumerate(products, 1):\n",
    "            response += f\"{i}. {product['title']}\\n\"\n",
    "            response += f\"   Total reviews: {product['reviews']}\\n\"\n",
    "            response += f\"   Average rating: {product['rating']}/5.0\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    elif \"similar\" in query.lower() or \"related\" in query.lower():\n",
    "        # Extract ASIN from query if present\n",
    "        asin_match = re.search(r'[A-Z0-9]{10}', query)\n",
    "        if asin_match:\n",
    "            asin = asin_match.group(0)\n",
    "            \n",
    "            # Find related products (variants or commonly bought together)\n",
    "            aql = \"\"\"\n",
    "            LET variants = (\n",
    "                FOR v, e IN 1..1 ANY @asin VariantOf\n",
    "                    RETURN v\n",
    "            )\n",
    "            \n",
    "            LET product = DOCUMENT(CONCAT('Products/', @asin))\n",
    "            \n",
    "            LET similar_products = (\n",
    "                FOR other IN Products\n",
    "                    FILTER other._key != @asin\n",
    "                    FILTER other.main_category == product.main_category\n",
    "                    SORT ABS(other.price - product.price) ASC\n",
    "                    LIMIT 3\n",
    "                    RETURN other\n",
    "            )\n",
    "            \n",
    "            RETURN {\n",
    "                variants: variants,\n",
    "                similar: similar_products,\n",
    "                original: product\n",
    "            }\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor = db.aql.execute(aql, bind_vars={\"asin\": asin})\n",
    "            result = next(cursor, None)\n",
    "            \n",
    "            if not result or not result.get('original'):\n",
    "                return f\"Could not find product with ASIN {asin}.\"\n",
    "            \n",
    "            response = f\"Analysis for product: {result['original'].get('title', 'Unknown')}\\n\\n\"\n",
    "            \n",
    "            if result['variants']:\n",
    "                response += \"Product variants:\\n\"\n",
    "                for i, variant in enumerate(result['variants'], 1):\n",
    "                    response += f\"{i}. {variant.get('title', 'Unknown')}\\n\"\n",
    "                    response += f\"   Price: ${variant.get('price', 0)}\\n\"\n",
    "                    response += f\"   ASIN: {variant.get('_key', 'Unknown')}\\n\\n\"\n",
    "            \n",
    "            if result['similar']:\n",
    "                response += \"Similar products by price and category:\\n\"\n",
    "                for i, similar in enumerate(result['similar'], 1):\n",
    "                    response += f\"{i}. {similar.get('title', 'Unknown')}\\n\"\n",
    "                    response += f\"   Price: ${similar.get('price', 0)}\\n\"\n",
    "                    response += f\"   ASIN: {similar.get('_key', 'Unknown')}\\n\\n\"\n",
    "            \n",
    "            return response\n",
    "        else:\n",
    "            return \"To find similar products, please provide a valid ASIN (10-character Amazon product ID).\"\n",
    "    \n",
    "    else:\n",
    "        # Default to general graph statistics\n",
    "        product_count = db.collection(\"Products\").count()\n",
    "        review_count = db.collection(\"Reviews\").count()\n",
    "        user_count = db.collection(\"Users\").count()\n",
    "        \n",
    "        return f\"\"\"\n",
    "        Graph Analytics Summary:\n",
    "        \n",
    "        - Total Products: {product_count}\n",
    "        - Total Reviews: {review_count}\n",
    "        - Total Users: {user_count}\n",
    "        \n",
    "        To get more specific analytics, try asking about:\n",
    "        - Popular or best-selling products\n",
    "        - Similar or related products (with an ASIN)\n",
    "        - Category trends\n",
    "        \"\"\"\n",
    "\n",
    "# Create the Agent\n",
    "def create_graph_rag_agent():\n",
    "    \"\"\"Create and return the GraphRAG agent\"\"\"\n",
    "    tools = [\n",
    "        get_product_by_description,\n",
    "        get_reviews_for_product,\n",
    "        text_to_aql_to_text,\n",
    "        analyze_product_network\n",
    "    ]\n",
    "    \n",
    "    return create_react_agent(llm, tools)\n",
    "\n",
    "# Function to query the agent\n",
    "def query_graph_rag(query):\n",
    "    \"\"\"Query the GraphRAG agent with a user question\"\"\"\n",
    "    agent = create_graph_rag_agent()\n",
    "    final_state = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "    return final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Testing the GraphRAG System\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find products similar to moisturizer\",\n",
    "    \"What are the reviews for product B088SZDGXG?\",\n",
    "    \"What's the most popular beauty product?\",\n",
    "    \"Tell me about products with the highest ratings\",\n",
    "    \"Find products under $10 with good reviews\"\n",
    "]\n",
    "\n",
    "# Uncomment to test the agent\n",
    "# for query in test_queries:\n",
    "#     print(f\"\\nQuery: {query}\")\n",
    "#     print(\"-\" * 50)\n",
    "#     response = query_graph_rag(query)\n",
    "#     print(response)\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "print(\"GraphRAG implementation complete! Run the test queries to see it in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Process Multiple Categories\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_categories(download_links, limit=None, batch_size=1000):\n",
    "    \"\"\"Process multiple categories with rate limiting and batch processing\"\"\"\n",
    "    categories_processed = 0\n",
    "    \n",
    "    # Create a processing queue from the download links\n",
    "    categories_to_process = list(download_links.keys())\n",
    "    if limit:\n",
    "        categories_to_process = categories_to_process[:limit]\n",
    "    \n",
    "    print(f\"Will process {len(categories_to_process)} categories\")\n",
    "    \n",
    "    for category in categories_to_process:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing category {categories_processed + 1}/{len(categories_to_process)}: {category}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get the download URL for this category\n",
    "        review_url = download_links[category]\n",
    "        \n",
    "        # Process the category data\n",
    "        reviews_data, metadata_data = process_category(category, review_url)\n",
    "        \n",
    "        # Skip if no data was loaded\n",
    "        if not reviews_data:\n",
    "            print(f\"No review data available for {category}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Transform the data\n",
    "        print(f\"Transforming data for {category}...\")\n",
    "        category_products = prepare_products_data(metadata_data)\n",
    "        category_reviews = prepare_reviews_data(reviews_data)\n",
    "        category_users = extract_users(category_reviews)\n",
    "        category_categories = extract_categories(category_products)\n",
    "        \n",
    "        # Create edge data\n",
    "        category_has_review_edges = create_has_review_edges(category_reviews)\n",
    "        category_written_by_edges = create_written_by_edges(category_reviews)\n",
    "        category_belongs_to_category_edges = create_belongs_to_category_edges(category_products)\n",
    "        category_variant_of_edges = create_variant_of_edges(category_products)\n",
    "        \n",
    "        # Insert data in batches\n",
    "        print(f\"Inserting data for {category} into ArangoDB...\")\n",
    "        \n",
    "        # Insert node data\n",
    "        print(\"Inserting product data...\")\n",
    "        insert_documents(\"Products\", category_products, batch_size)\n",
    "        \n",
    "        print(\"Inserting review data...\")\n",
    "        insert_documents(\"Reviews\", category_reviews, batch_size)\n",
    "        \n",
    "        print(\"Inserting user data...\")\n",
    "        insert_documents(\"Users\", category_users, batch_size)\n",
    "        \n",
    "        print(\"Inserting category data...\")\n",
    "        insert_documents(\"Categories\", category_categories, batch_size)\n",
    "        \n",
    "        # Insert edge data\n",
    "        print(\"Creating product-review connections...\")\n",
    "        insert_documents(\"HasReview\", category_has_review_edges, batch_size)\n",
    "        \n",
    "        print(\"Creating review-user connections...\")\n",
    "        insert_documents(\"WrittenBy\", category_written_by_edges, batch_size)\n",
    "        \n",
    "        print(\"Creating product-category connections...\")\n",
    "        insert_documents(\"BelongsToCategory\", category_belongs_to_category_edges, batch_size)\n",
    "        \n",
    "        print(\"Creating product variant connections...\")\n",
    "        insert_documents(\"VariantOf\", category_variant_of_edges, batch_size)\n",
    "        \n",
    "        # Generate embeddings for this category\n",
    "        if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            print(f\"Generating embeddings for {category} products and reviews...\")\n",
    "            generate_category_embeddings(category, batch_size=25)\n",
    "        \n",
    "        # Increment the counter\n",
    "        categories_processed += 1\n",
    "        \n",
    "        # Add a delay between categories to avoid overwhelming the database\n",
    "        if categories_processed < len(categories_to_process):\n",
    "            delay = 5  # seconds\n",
    "            print(f\"Waiting {delay} seconds before processing the next category...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(f\"\\nProcessed {categories_processed}/{len(categories_to_process)} categories successfully\")\n",
    "\n",
    "def generate_category_embeddings(category, batch_size=25):\n",
    "    \"\"\"Generate embeddings for products and reviews of a specific category\"\"\"\n",
    "    # Query products in this category\n",
    "    aql_products = \"\"\"\n",
    "    FOR p IN Products\n",
    "        FILTER p.main_category == @category AND NOT HAS(p, \"embedding\")\n",
    "        LIMIT @batch_size\n",
    "        RETURN p\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process products in batches\n",
    "    while True:\n",
    "        products_batch = list(db.aql.execute(aql_products, bind_vars={\"category\": category, \"batch_size\": batch_size}))\n",
    "        if not products_batch:\n",
    "            break\n",
    "            \n",
    "        # Prepare text for embedding\n",
    "        product_texts = []\n",
    "        product_keys = []\n",
    "        for product in products_batch:\n",
    "            # Combine title, features, and description\n",
    "            text = f\"{product.get('title', '')} {product.get('features_text', '')} {product.get('description', '')}\"\n",
    "            product_texts.append(text)\n",
    "            product_keys.append(product[\"_key\"])\n",
    "        \n",
    "        # Generate embeddings\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(product_texts)\n",
    "            \n",
    "            # Update products with embeddings\n",
    "            for i, key in enumerate(product_keys):\n",
    "                db.collection(\"Products\").update({\n",
    "                    \"_key\": key,\n",
    "                    \"embedding\": embeddings[i]\n",
    "                })\n",
    "            print(f\"Added embeddings to {len(product_keys)} products\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating product embeddings: {e}\")\n",
    "    \n",
    "    # Query reviews for products in this category\n",
    "    aql_reviews = \"\"\"\n",
    "    FOR r IN Reviews\n",
    "        FILTER EXISTS(FOR p IN Products FILTER p._key == r.asin AND p.main_category == @category RETURN 1)\n",
    "        AND NOT HAS(r, \"embedding\")\n",
    "        LIMIT @batch_size\n",
    "        RETURN r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process reviews in batches\n",
    "    while True:\n",
    "        reviews_batch = list(db.aql.execute(aql_reviews, bind_vars={\"category\": category, \"batch_size\": batch_size}))\n",
    "        if not reviews_batch:\n",
    "            break\n",
    "            \n",
    "        # Prepare text for embedding\n",
    "        review_texts = []\n",
    "        review_keys = []\n",
    "        for review in reviews_batch:\n",
    "            text = f\"{review.get('title', '')} {review.get('text', '')}\"\n",
    "            review_texts.append(text)\n",
    "            review_keys.append(review[\"_key\"])\n",
    "        \n",
    "        # Generate embeddings\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(review_texts)\n",
    "            \n",
    "            # Update reviews with embeddings\n",
    "            for i, key in enumerate(review_keys):\n",
    "                db.collection(\"Reviews\").update({\n",
    "                    \"_key\": key,\n",
    "                    \"embedding\": embeddings[i]\n",
    "                })\n",
    "            print(f\"Added embeddings to {len(review_keys)} reviews\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating review embeddings: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Visualize a subset of the graph\n",
    "def visualize_graph_sample():\n",
    "    \"\"\"Visualize a small sample of the graph\"\"\"\n",
    "    # Get a sample of products\n",
    "    products = list(db.collection(\"Products\").all().limit(5))\n",
    "    product_asins = [p[\"_key\"] for p in products]\n",
    "    \n",
    "    # Create a small NetworkX graph for visualization\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add product nodes\n",
    "    for product in products:\n",
    "        G.add_node(product[\"_key\"], type=\"product\", label=product[\"title\"][:20])\n",
    "    \n",
    "    # Get reviews for these products\n",
    "    for asin in product_asins:\n",
    "        reviews = db.aql.execute(\n",
    "            \"FOR r IN Reviews FILTER r.asin == @asin LIMIT 3 RETURN r\",\n",
    "            bind_vars={\"asin\": asin}\n",
    "        )\n",
    "        \n",
    "        # Add review nodes and edges\n",
    "        for review in reviews:\n",
    "            G.add_node(review[\"_key\"], type=\"review\", label=\"Review\")\n",
    "            G.add_edge(asin, review[\"_key\"], type=\"has_review\")\n",
    "            \n",
    "            # Add user node and edge\n",
    "            user_id = review[\"user_id\"]\n",
    "            G.add_node(user_id, type=\"user\", label=f\"User\")\n",
    "            G.add_edge(review[\"_key\"], user_id, type=\"written_by\")\n",
    "    \n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw nodes with different colors by type\n",
    "    node_colors = {\n",
    "        \"product\": \"skyblue\",\n",
    "        \"review\": \"lightgreen\",\n",
    "        \"user\": \"salmon\"\n",
    "    }\n",
    "    \n",
    "    for node_type, color in node_colors.items():\n",
    "        nodes = [n for n, data in G.nodes(data=True) if data.get(\"type\") == node_type]\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=color, node_size=300)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {n: data.get(\"label\", n) for n, data in G.nodes(data=True)}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title(\"Sample of Amazon Reviews Graph\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Process all categories (comment this out for testing or to process just a subset)\n",
    "# process_all_categories(download_links, limit=5)  # Process just the first 5 categories\n",
    "\n",
    "# To process just one category for testing\n",
    "# process_all_categories({CATEGORY: download_links[CATEGORY]})\n",
    "\n",
    "# Uncomment to visualize a sample of the graph\n",
    "# visualize_graph_sample()\n",
    "\n",
    "print(\"Script is ready to process all Amazon product categories.\")\n",
    "print(\"To run the full processing, uncomment the process_all_categories() line.\")\n",
    "print(\"You can set a limit to process only a subset of categories for testing.\")\n",
    "print(\"Make sure to set your OpenAI API key for embedding generation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-max",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
