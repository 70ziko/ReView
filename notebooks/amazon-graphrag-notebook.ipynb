{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews GraphRAG Implementation\n",
    "\n",
    "This notebook implements a GraphRAG system for Amazon product reviews using ArangoDB\n",
    "for graph storage and querying, and LangChain/LangGraph for the agentic components.\n",
    "\n",
    "# Step 0: Package Installation & Setup\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nx-arangodb\n",
      "  Downloading nx_arangodb-1.3.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m659.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain\n",
      "  Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m878.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-community\n",
      "  Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m695.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m467.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langgraph\n",
      "  Downloading langgraph-0.3.5-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m484.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx<=3.4,>=3.0\n",
      "  Downloading networkx-3.4-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m968.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting phenolrs~=0.5\n",
      "  Downloading phenolrs-0.5.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m690.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-arango~=8.1\n",
      "  Downloading python_arango-8.1.6-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m709.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting adbnx-adapter~=5.0.5\n",
      "  Downloading adbnx_adapter-5.0.6-py3-none-any.whl (21 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.41\n",
      "  Downloading langchain_core-0.3.41-py3-none-any.whl (415 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m665.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<1.0.0,>=0.3.6\n",
      "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17\n",
      "  Downloading langsmith-0.3.12-py3-none-any.whl (335 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m523.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3.0.0,>=2.7.4\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m585.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m472.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests<3,>=2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m424.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML>=5.3\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m610.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m575.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity!=8.4.0,<10,>=8.1.0\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /home/paris/.local/lib/python3.11/site-packages (from langchain-community) (1.26.2)\n",
      "Collecting openai<2.0.0,>=1.58.1\n",
      "  Downloading openai-1.65.4-py3-none-any.whl (473 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.5/473.5 kB\u001b[0m \u001b[31m622.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m564.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-checkpoint<3.0.0,>=2.0.10\n",
      "  Downloading langgraph_checkpoint-2.0.17-py3-none-any.whl (39 kB)\n",
      "Collecting langgraph-prebuilt<0.2,>=0.1.1\n",
      "  Downloading langgraph_prebuilt-0.1.2-py3-none-any.whl (24 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42\n",
      "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m434.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich>=12.5.1\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m457.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=45 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (66.0.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m313.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m414.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m360.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m598.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m379.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m411.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Collecting msgpack<2.0.0,>=1.1.0\n",
      "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.7/403.7 kB\u001b[0m \u001b[31m395.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.25.2\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m400.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.10.1\n",
      "  Downloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m335.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m255.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m344.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m448.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m338.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m162.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m260.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv>=0.21.0\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m357.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyJWT\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: importlib_metadata>=4.7.1 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m300.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m387.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m320.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.4/602.4 kB\u001b[0m \u001b[31m266.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m68.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m134.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m89.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.19.1)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: zstandard, urllib3, tqdm, tenacity, sniffio, regex, PyYAML, python-dotenv, PyJWT, pydantic-core, propcache, orjson, networkx, mypy-extensions, multidict, msgpack, mdurl, marshmallow, jsonpointer, jiter, idna, httpx-sse, h11, greenlet, frozenlist, distro, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, markdown-it-py, jsonpatch, httpcore, anyio, aiosignal, tiktoken, rich, requests-toolbelt, pydantic-settings, httpx, dataclasses-json, aiohttp, python-arango, openai, langsmith, langgraph-sdk, phenolrs, langchain-core, adbnx-adapter, nx-arangodb, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain, langgraph, langchain-community\n",
      "Successfully installed PyJWT-2.10.1 PyYAML-6.0.2 SQLAlchemy-2.0.38 adbnx-adapter-5.0.6 aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 dataclasses-json-0.6.7 distro-1.9.0 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 idna-3.10 jiter-0.8.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.20 langchain-community-0.3.19 langchain-core-0.3.41 langchain-openai-0.3.7 langchain-text-splitters-0.3.6 langgraph-0.3.5 langgraph-checkpoint-2.0.17 langgraph-prebuilt-0.1.2 langgraph-sdk-0.1.53 langsmith-0.3.12 markdown-it-py-3.0.0 marshmallow-3.26.1 mdurl-0.1.2 msgpack-1.1.0 multidict-6.1.0 mypy-extensions-1.0.0 networkx-3.4 nx-arangodb-1.3.0 openai-1.65.4 orjson-3.10.15 phenolrs-0.5.9 propcache-0.3.0 pydantic-2.10.6 pydantic-core-2.27.2 pydantic-settings-2.8.1 python-arango-8.1.6 python-dotenv-1.0.1 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 rich-13.9.4 sniffio-1.3.1 tenacity-9.0.0 tiktoken-0.9.0 tqdm-4.67.1 typing-inspect-0.9.0 urllib3-2.3.0 yarl-1.18.3 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/paris/.local/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /home/paris/.local/lib/python3.11/site-packages (1.26.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m272.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: openai in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (1.65.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/paris/.local/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m813.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /home/paris/.local/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m605.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m642.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m743.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m756.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m399.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.1 pytz-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nx-arangodb langchain langchain-community langchain-openai langgraph\n",
    "%pip install pandas numpy matplotlib tqdm openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/paris/anaconda3/envs/autoAI/lib/python3.11/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:15:36 +0100] [INFO]: NetworkX-cuGraph is unavailable: No module named 'cupy'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import networkx as nx\n",
    "import nx_arangodb as nxadb\n",
    "import matplotlib.pyplot as plt\n",
    "from arango import ArangoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# For the agentic components\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ArangoDB: 8.1.5\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Connect to the ArangoDB database\n",
    "client = ArangoClient(hosts=\"https://17c43fa12737.arangodb.cloud:8529\")\n",
    "db = client.db(\"_system\", username=os.getenv(\"ARANGO_DB_USER\"), password=os.getenv(\"ARANGO_DB_PASS\"), verify=True)\n",
    "\n",
    "print(f\"Connected to ArangoDB: {client.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load and Transform the Amazon Review Data\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "\n",
    "def download_file(url, target_path):\n",
    "    \"\"\"Download a file from URL to target path with progress bar\"\"\"\n",
    "    os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"File already exists: {target_path}\")\n",
    "        return target_path\n",
    "    \n",
    "    print(f\"Downloading {url} to {target_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Disable SSL verification\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        \n",
    "        # Set up progress bar\n",
    "        response = urllib.request.urlopen(url)\n",
    "        file_size = int(response.headers.get('Content-Length', 0))\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(total=file_size, unit='B', unit_scale=True, desc=os.path.basename(target_path))\n",
    "        \n",
    "        # Download with progress updates\n",
    "        with open(target_path, 'wb') as f:\n",
    "            block_size = 8192\n",
    "            while True:\n",
    "                buffer = response.read(block_size)\n",
    "                if not buffer:\n",
    "                    break\n",
    "                f.write(buffer)\n",
    "                progress.update(len(buffer))\n",
    "                \n",
    "        progress.close()\n",
    "        print(f\"Downloaded {target_path}\")\n",
    "        return target_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_gzip(gzip_path, extract_path=None):\n",
    "    \"\"\"Extract a gzip file to the specified path\"\"\"\n",
    "    if extract_path is None:\n",
    "        # Remove .gz extension if present\n",
    "        extract_path = gzip_path[:-3] if gzip_path.endswith('.gz') else gzip_path + '_extracted'\n",
    "    \n",
    "    if os.path.exists(extract_path):\n",
    "        print(f\"Extracted file already exists: {extract_path}\")\n",
    "        return extract_path\n",
    "    \n",
    "    print(f\"Extracting {gzip_path} to {extract_path}\")\n",
    "    try:\n",
    "        with gzip.open(gzip_path, 'rb') as f_in:\n",
    "            with open(extract_path, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        print(f\"Extracted to {extract_path}\")\n",
    "        return extract_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {gzip_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_json_data(file_path, limit=None):\n",
    "    \"\"\"Load JSON data from file, line by line\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON on line {i+1}\")\n",
    "    return data\n",
    "\n",
    "def parse_download_links(file_path):\n",
    "    \"\"\"Parse download links from file\"\"\"\n",
    "    links = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(': ')\n",
    "            if len(parts) == 2:\n",
    "                category, url = parts\n",
    "                links[category] = url\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 category download links\n"
     ]
    }
   ],
   "source": [
    "# Create data directories\n",
    "DATA_DIR = \"amazon_data\"\n",
    "DOWNLOAD_DIR = os.path.join(DATA_DIR, \"downloaded\")\n",
    "EXTRACTED_DIR = os.path.join(DATA_DIR, \"extracted\")\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACTED_DIR, exist_ok=True)\n",
    "\n",
    "# Parse download links\n",
    "download_links = parse_download_links(\"dataset_urls.txt\")\n",
    "print(f\"Found {len(download_links)} category download links\")\n",
    "\n",
    "# Metadata URL pattern\n",
    "# Assuming metadata URLs follow the same pattern with 'meta_' prefix\n",
    "META_URL_BASE = \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process a single category\n",
    "def process_category(category, review_url, sample_size=1000, meta_sample_size=500):\n",
    "    \"\"\"Download, extract, and load data for a single category\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing category: {category}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Determine file paths\n",
    "    review_gz_path = os.path.join(DOWNLOAD_DIR, f\"{category}.jsonl.gz\")\n",
    "    review_path = os.path.join(EXTRACTED_DIR, f\"{category}.jsonl\")\n",
    "    \n",
    "    # For metadata, construct the URL and paths\n",
    "    meta_url = META_URL_BASE + category + \".jsonl.gz\"\n",
    "    meta_gz_path = os.path.join(DOWNLOAD_DIR, f\"meta_{category}.jsonl.gz\")\n",
    "    meta_path = os.path.join(EXTRACTED_DIR, f\"meta_{category}.jsonl\")\n",
    "    \n",
    "    # Download and extract review data\n",
    "    download_file(review_url, review_gz_path)\n",
    "    extract_gzip(review_gz_path, review_path)\n",
    "    \n",
    "    # Download and extract metadata\n",
    "    try:\n",
    "        download_file(meta_url, meta_gz_path)\n",
    "        extract_gzip(meta_gz_path, meta_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading metadata for {category}: {e}\")\n",
    "        print(\"Continuing without metadata for this category\")\n",
    "        meta_path = None\n",
    "    \n",
    "    # Load review data (with limit for testing)\n",
    "    print(f\"Loading review data for {category}...\")\n",
    "    reviews_data = load_json_data(review_path, limit=sample_size)\n",
    "    print(f\"Loaded {len(reviews_data)} reviews\")\n",
    "    \n",
    "    # Load metadata if available\n",
    "    metadata_data = []\n",
    "    if meta_path and os.path.exists(meta_path):\n",
    "        print(f\"Loading product metadata for {category}...\")\n",
    "        metadata_data = load_json_data(meta_path, limit=meta_sample_size)\n",
    "        print(f\"Loaded {len(metadata_data)} product metadata entries\")\n",
    "    \n",
    "    return reviews_data, metadata_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process categories\n",
    "For testing, we'll just process one category first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process 1 categories: ['All_Beauty']\n",
      "\n",
      "==================================================\n",
      "Processing category: All_Beauty\n",
      "==================================================\n",
      "File already exists: amazon_data/downloaded/All_Beauty.jsonl.gz\n",
      "Extracted file already exists: amazon_data/extracted/All_Beauty.jsonl\n",
      "File already exists: amazon_data/downloaded/meta_All_Beauty.jsonl.gz\n",
      "Extracted file already exists: amazon_data/extracted/meta_All_Beauty.jsonl\n",
      "Loading review data for All_Beauty...\n",
      "Loaded 1000 reviews\n",
      "Loading product metadata for All_Beauty...\n",
      "Loaded 500 product metadata entries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nall_reviews = {}\\nall_metadata = {}\\nfor category, url in download_links.items():\\n    reviews, metadata = process_category(category, url)\\n    all_reviews[category] = reviews\\n    all_metadata[category] = metadata\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can modify this to process all categories or a subset\n",
    "CATEGORIES_TO_PROCESS = list(download_links.keys())[:1]  # Start with just one for testing\n",
    "print(f\"Will process {len(CATEGORIES_TO_PROCESS)} categories: {CATEGORIES_TO_PROCESS}\")\n",
    "\n",
    "# For demonstration, process just the first category\n",
    "CATEGORY = CATEGORIES_TO_PROCESS[0]\n",
    "reviews_data, metadata_data = process_category(CATEGORY, download_links[CATEGORY])\n",
    "\n",
    "# You can expand this to process all categories sequentially or in parallel\n",
    "# Example for processing all categories:\n",
    "\"\"\"\n",
    "all_reviews = {}\n",
    "all_metadata = {}\n",
    "for category, url in download_links.items():\n",
    "    reviews, metadata = process_category(category, url)\n",
    "    all_reviews[category] = reviews\n",
    "    all_metadata[category] = metadata\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Create and Configure the ArangoDB Graph\n",
    "-----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists: Products\n",
      "Collection already exists: Reviews\n",
      "Collection already exists: Users\n",
      "Collection already exists: Categories\n",
      "Edge collection already exists: HasReview\n",
      "Edge collection already exists: WrittenBy\n",
      "Edge collection already exists: BelongsToCategory\n",
      "Edge collection already exists: VariantOf\n",
      "Graph already exists: AmazonReviews\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the collections needed for our graph\n",
    "GRAPH_NAME = \"AmazonReviews\"\n",
    "COLLECTIONS = {\n",
    "    \"nodes\": [\"Products\", \"Reviews\", \"Users\", \"Categories\"],\n",
    "    \"edges\": [\"HasReview\", \"WrittenBy\", \"BelongsToCategory\", \"VariantOf\"]\n",
    "}\n",
    "\n",
    "# Create collections if they don't exist\n",
    "def ensure_collections_exist():\n",
    "    \"\"\"Create the required collections if they don't exist\"\"\"\n",
    "    # Create node collections\n",
    "    for collection in COLLECTIONS[\"nodes\"]:\n",
    "        if not db.has_collection(collection):\n",
    "            db.create_collection(collection)\n",
    "            print(f\"Created collection: {collection}\")\n",
    "            \n",
    "            # Create indexes for faster queries\n",
    "            if collection == \"Products\":\n",
    "                # Index on ASIN (product ID)\n",
    "                db.collection(collection).add_hash_index([\"_key\"], unique=True)\n",
    "                # Index on main category for category-based queries\n",
    "                db.collection(collection).add_hash_index([\"main_category\"], unique=False)\n",
    "                # Index on product parent ASIN for variant relationships\n",
    "                db.collection(collection).add_hash_index([\"parent_asin\"], unique=False)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "                \n",
    "            elif collection == \"Reviews\":\n",
    "                # Index on product ASIN for finding reviews of a product\n",
    "                db.collection(collection).add_hash_index([\"asin\"], unique=False)\n",
    "                # Index on user ID for finding reviews by a user\n",
    "                db.collection(collection).add_hash_index([\"user_id\"], unique=False)\n",
    "                # Index on timestamp for chronological queries\n",
    "                db.collection(collection).add_skiplist_index([\"timestamp\"], unique=False)\n",
    "                # Index on rating for filtering by rating\n",
    "                db.collection(collection).add_skiplist_index([\"rating\"], unique=False)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "                \n",
    "            elif collection == \"Users\":\n",
    "                # Index on user ID\n",
    "                db.collection(collection).add_hash_index([\"_key\"], unique=True)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "                \n",
    "            elif collection == \"Categories\":\n",
    "                # Index on category name\n",
    "                db.collection(collection).add_hash_index([\"_key\"], unique=True)\n",
    "                # Index on category level for hierarchical queries\n",
    "                db.collection(collection).add_skiplist_index([\"level\"], unique=False)\n",
    "                print(f\"Created indexes for {collection}\")\n",
    "        else:\n",
    "            print(f\"Collection already exists: {collection}\")\n",
    "    \n",
    "    # Create edge collections\n",
    "    for collection in COLLECTIONS[\"edges\"]:\n",
    "        if not db.has_collection(collection):\n",
    "            db.create_collection(collection, edge=True)\n",
    "            print(f\"Created edge collection: {collection}\")\n",
    "        else:\n",
    "            print(f\"Edge collection already exists: {collection}\")\n",
    "\n",
    "# Create the graph if it doesn't exist\n",
    "def ensure_graph_exists():\n",
    "    \"\"\"Create the graph if it doesn't exist\"\"\"\n",
    "    if not db.has_graph(GRAPH_NAME):\n",
    "        graph = db.create_graph(GRAPH_NAME)\n",
    "        \n",
    "        # Define edge definitions\n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"HasReview\",\n",
    "            from_vertex_collections=[\"Products\"],\n",
    "            to_vertex_collections=[\"Reviews\"]\n",
    "        )\n",
    "        \n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"WrittenBy\",\n",
    "            from_vertex_collections=[\"Reviews\"],\n",
    "            to_vertex_collections=[\"Users\"]\n",
    "        )\n",
    "        \n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"BelongsToCategory\",\n",
    "            from_vertex_collections=[\"Products\"],\n",
    "            to_vertex_collections=[\"Categories\"]\n",
    "        )\n",
    "        \n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"VariantOf\",\n",
    "            from_vertex_collections=[\"Products\"],\n",
    "            to_vertex_collections=[\"Products\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"Created graph: {GRAPH_NAME}\")\n",
    "    else:\n",
    "        print(f\"Graph already exists: {GRAPH_NAME}\")\n",
    "\n",
    "# Create collections and graph\n",
    "ensure_collections_exist()\n",
    "ensure_graph_exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Transform and Insert the Data\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 metadata items\n",
      "Sample metadata item structure: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'details', 'parent_asin', 'bought_together']\n",
      "Transformed 500 products (skipped 0 items)\n",
      "Processing 1000 review items\n",
      "Sample review item structure: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n",
      "Transformed 1000 reviews (skipped 0 items)\n",
      "Extracting categories from 500 products\n",
      "Extracted 2 unique categories\n",
      "Inserting product data...\n",
      "Successfully inserted/updated 500 documents in Products\n",
      "Inserting review data...\n",
      "Successfully inserted/updated 1000 documents in Reviews\n",
      "Inserting user data...\n",
      "Successfully inserted/updated 415 documents in Users\n",
      "Inserting category data...\n",
      "Successfully inserted/updated 2 documents in Categories\n",
      "Created 10 HasReview edges (out of 1000 reviews)\n",
      "Created 1000 WrittenBy edges (out of 1000 reviews)\n",
      "Created 500 BelongsToCategory edges (out of 500 products)\n",
      "Created 0 VariantOf edges (out of 500 products)\n",
      "Creating product-review connections...\n",
      "Successfully inserted/updated 10 documents in HasReview\n",
      "Creating review-user connections...\n",
      "Successfully inserted/updated 1000 documents in WrittenBy\n",
      "Creating product-category connections...\n",
      "Successfully inserted/updated 500 documents in BelongsToCategory\n",
      "Creating product variant connections...\n",
      "No documents to insert into VariantOf\n"
     ]
    }
   ],
   "source": [
    "# Function to sanitize keys for ArangoDB\n",
    "def sanitize_key(key, allow_spaces=False):\n",
    "    \"\"\"Sanitize a string to be used as a key in ArangoDB\"\"\"\n",
    "    if not key:\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    key = str(key)\n",
    "    \n",
    "    # Replace problematic characters\n",
    "    key = key.replace('/', '_').replace('\\\\', '_').replace('.', '_')\n",
    "    \n",
    "    # Replace spaces if not allowed\n",
    "    if not allow_spaces:\n",
    "        key = key.replace(' ', '_')\n",
    "    \n",
    "    # Remove any other potentially problematic characters\n",
    "    # ArangoDB keys must be URL-safe\n",
    "    import re\n",
    "    key = re.sub(r'[^a-zA-Z0-9_\\-]', '_', key)\n",
    "    \n",
    "    # Ensure the key is not empty and doesn't start with a number or underscore\n",
    "    if not key or key[0].isdigit() or key[0] == '_':\n",
    "        key = 'a' + key\n",
    "    \n",
    "    return key\n",
    "\n",
    "# Prepare products data\n",
    "def prepare_products_data(metadata_list):\n",
    "    \"\"\"Transform metadata into a format suitable for Products collection\"\"\"\n",
    "    print(f\"Processing {len(metadata_list)} metadata items\")\n",
    "    \n",
    "    # Debug: Print a sample item to see its structure\n",
    "    if metadata_list and len(metadata_list) > 0:\n",
    "        print(\"Sample metadata item structure:\", list(metadata_list[0].keys()))\n",
    "    \n",
    "    products = []\n",
    "    skipped_items = 0\n",
    "    \n",
    "    for item in metadata_list:\n",
    "        # Use parent_asin as the primary identifier\n",
    "        # In the Amazon data, this appears to be the product ID\n",
    "        item_id = item.get('parent_asin')\n",
    "        \n",
    "        # Skip items without an identifier\n",
    "        if not item_id:\n",
    "            skipped_items += 1\n",
    "            continue\n",
    "            \n",
    "        # Sanitize the ID for use as key\n",
    "        asin = sanitize_key(item_id)\n",
    "        if not asin:\n",
    "            skipped_items += 1\n",
    "            continue\n",
    "        \n",
    "        # Combine description into a single string if it's a list\n",
    "        description = ''\n",
    "        if isinstance(item.get('description', []), list):\n",
    "            # Filter out None values and join\n",
    "            desc_items = [d for d in item.get('description', []) if d]\n",
    "            description = ' '.join(desc_items)\n",
    "        else:\n",
    "            description = str(item.get('description', ''))\n",
    "        \n",
    "        # Extract features as a list and handle potential None values\n",
    "        features = []\n",
    "        if isinstance(item.get('features', []), list):\n",
    "            features = [f for f in item.get('features', []) if f]\n",
    "        \n",
    "        # Join features for text search\n",
    "        features_text = ' '.join(features) if features else ''\n",
    "        \n",
    "        # Sanitize main_category for use in category edges\n",
    "        main_category = item.get('main_category', '')\n",
    "        if main_category:\n",
    "            main_category = sanitize_key(main_category, allow_spaces=True)\n",
    "        \n",
    "        # Extract image URLs, handling potential None values\n",
    "        images = []\n",
    "        if isinstance(item.get('images', []), list):\n",
    "            for img in item.get('images', []):\n",
    "                if isinstance(img, dict) and 'large' in img and img['large']:\n",
    "                    images.append(img['large'])\n",
    "                    if len(images) >= 3:\n",
    "                        break\n",
    "        \n",
    "        # Handle potential None or invalid values for numeric fields\n",
    "        try:\n",
    "            price = float(item.get('price', 0)) if item.get('price') is not None else 0\n",
    "        except (ValueError, TypeError):\n",
    "            price = 0\n",
    "            \n",
    "        try:\n",
    "            avg_rating = float(item.get('average_rating', 0)) if item.get('average_rating') is not None else 0\n",
    "        except (ValueError, TypeError):\n",
    "            avg_rating = 0\n",
    "            \n",
    "        try:\n",
    "            rating_count = int(item.get('rating_number', 0)) if item.get('rating_number') is not None else 0\n",
    "        except (ValueError, TypeError):\n",
    "            rating_count = 0\n",
    "        \n",
    "        # Prepare product document with sanitized values\n",
    "        product = {\n",
    "            \"_key\": asin,\n",
    "            \"original_id\": item_id,  # Keep original ID for reference\n",
    "            \"title\": str(item.get('title', '')),\n",
    "            \"main_category\": main_category,\n",
    "            \"description\": description,\n",
    "            \"features\": features,\n",
    "            \"features_text\": features_text,\n",
    "            \"price\": price,\n",
    "            \"average_rating\": avg_rating,\n",
    "            \"rating_count\": rating_count,\n",
    "            \"store\": str(item.get('store', '')),\n",
    "            \"details\": item.get('details', {}),\n",
    "            \"parent_asin\": asin, \n",
    "            \"bought_together\": item.get('bought_together', []),\n",
    "            \"images\": images\n",
    "        }\n",
    "        products.append(product)\n",
    "    \n",
    "    print(f\"Transformed {len(products)} products (skipped {skipped_items} items)\")\n",
    "    return products\n",
    "\n",
    "# Prepare reviews data\n",
    "def prepare_reviews_data(reviews_list):\n",
    "    \"\"\"Transform reviews into a format suitable for Reviews collection\"\"\"\n",
    "    print(f\"Processing {len(reviews_list)} review items\")\n",
    "    \n",
    "    # Debug: Print a sample item structure\n",
    "    if reviews_list and len(reviews_list) > 0:\n",
    "        print(\"Sample review item structure:\", list(reviews_list[0].keys()))\n",
    "    \n",
    "    reviews = []\n",
    "    skipped_items = 0\n",
    "    \n",
    "    for item in reviews_list:\n",
    "        # Check if we have necessary fields\n",
    "        asin_value = item.get('asin')\n",
    "        user_id_value = item.get('user_id')\n",
    "        \n",
    "        if not asin_value or not user_id_value:\n",
    "            skipped_items += 1\n",
    "            continue\n",
    "        \n",
    "        # Sanitize the ASIN and user_id\n",
    "        asin = sanitize_key(asin_value)\n",
    "        user_id = sanitize_key(user_id_value)\n",
    "        \n",
    "        if not asin or not user_id:\n",
    "            skipped_items += 1\n",
    "            continue\n",
    "        \n",
    "        # Create a unique key for the review\n",
    "        timestamp = item.get('sort_timestamp', 0)\n",
    "        # Ensure timestamp is a number\n",
    "        if not timestamp:\n",
    "            timestamp = 0\n",
    "        try:\n",
    "            timestamp = int(timestamp)\n",
    "        except (ValueError, TypeError):\n",
    "            timestamp = 0\n",
    "            \n",
    "        # Create a unique but valid key\n",
    "        key = f\"{asin}_{user_id}_{timestamp}\"\n",
    "        key = sanitize_key(key)\n",
    "        \n",
    "        # Extract image URLs if available\n",
    "        images = []\n",
    "        if isinstance(item.get('images', []), list):\n",
    "            for img in item.get('images', []):\n",
    "                if isinstance(img, dict) and 'large_image_url' in img and img['large_image_url']:\n",
    "                    images.append(img['large_image_url'])\n",
    "        \n",
    "        # Sanitize parent_asin\n",
    "        parent_asin = sanitize_key(item.get('parent_asin', ''))\n",
    "        \n",
    "        # Handle potential None or invalid values for numeric fields\n",
    "        try:\n",
    "            rating = float(item.get('rating', 0)) if item.get('rating') is not None else 0\n",
    "        except (ValueError, TypeError):\n",
    "            rating = 0\n",
    "            \n",
    "        try:\n",
    "            helpful_votes = int(item.get('helpful_votes', 0)) if item.get('helpful_votes') is not None else 0\n",
    "        except (ValueError, TypeError):\n",
    "            helpful_votes = 0\n",
    "        \n",
    "        # Prepare review document with sanitized values\n",
    "        review = {\n",
    "            \"_key\": key,\n",
    "            \"asin\": asin,\n",
    "            \"original_asin\": asin_value,  # Keep original for reference\n",
    "            \"user_id\": user_id,\n",
    "            \"original_user_id\": user_id_value,  # Keep original for reference\n",
    "            \"parent_asin\": parent_asin,\n",
    "            \"original_parent_asin\": item.get('parent_asin', ''),  # Keep original for reference\n",
    "            \"rating\": rating,\n",
    "            \"title\": str(item.get('title', '')),\n",
    "            \"text\": str(item.get('text', '')),\n",
    "            \"timestamp\": timestamp,\n",
    "            \"helpful_votes\": helpful_votes,\n",
    "            \"verified_purchase\": bool(item.get('verified_purchase', False)),\n",
    "            \"images\": images\n",
    "        }\n",
    "        reviews.append(review)\n",
    "    \n",
    "    print(f\"Transformed {len(reviews)} reviews (skipped {skipped_items} items)\")\n",
    "    return reviews\n",
    "\n",
    "# Extract users from reviews\n",
    "def extract_users(reviews_list):\n",
    "    \"\"\"Extract unique users from reviews data\"\"\"\n",
    "    users = {}\n",
    "    for review in reviews_list:\n",
    "        user_id = review.get('user_id', '')\n",
    "        original_user_id = review.get('original_user_id', '')\n",
    "        \n",
    "        if user_id and user_id not in users:\n",
    "            users[user_id] = {\n",
    "                \"_key\": user_id,\n",
    "                \"original_user_id\": original_user_id,\n",
    "                \"review_count\": 1\n",
    "            }\n",
    "        elif user_id:\n",
    "            users[user_id][\"review_count\"] += 1\n",
    "    return list(users.values())\n",
    "\n",
    "# Extract categories from products\n",
    "def extract_categories(products_list):\n",
    "    \"\"\"Extract unique categories from product metadata\"\"\"\n",
    "    print(f\"Extracting categories from {len(products_list)} products\")\n",
    "    \n",
    "    categories = {}\n",
    "    for product in products_list:\n",
    "        main_category = product.get('main_category', '')\n",
    "        \n",
    "        if main_category and main_category not in categories:\n",
    "            # Sanitize key for ArangoDB\n",
    "            category_key = sanitize_key(main_category)\n",
    "            \n",
    "            categories[main_category] = {\n",
    "                \"_key\": category_key,\n",
    "                \"name\": main_category,\n",
    "                \"level\": 0  # Top level category\n",
    "            }\n",
    "        \n",
    "        # Process hierarchical categories if available\n",
    "        if 'categories' in product and isinstance(product['categories'], list) and product['categories']:\n",
    "            for i, category_list in enumerate(product['categories']):\n",
    "                if isinstance(category_list, list) and category_list:\n",
    "                    for j, category in enumerate(category_list):\n",
    "                        if category and category not in categories:\n",
    "                            # Sanitize key for ArangoDB\n",
    "                            category_key = sanitize_key(category)\n",
    "                            \n",
    "                            categories[category] = {\n",
    "                                \"_key\": category_key,\n",
    "                                \"name\": category,\n",
    "                                \"level\": j+1  # Level in hierarchy\n",
    "                            }\n",
    "    \n",
    "    categories_list = list(categories.values())\n",
    "    print(f\"Extracted {len(categories_list)} unique categories\")\n",
    "    return categories_list\n",
    "\n",
    "# Create edge data\n",
    "def create_has_review_edges(reviews):\n",
    "    \"\"\"Create edges connecting products to reviews\"\"\"\n",
    "    edges = []\n",
    "    product_keys = set()  # Keep track of valid product keys\n",
    "    \n",
    "    # Get all valid product keys from the database\n",
    "    try:\n",
    "        products_cursor = db.collection(\"Products\").all()\n",
    "        for product in products_cursor:\n",
    "            product_keys.add(product[\"_key\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching product keys: {e}\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        product_key = review['asin']\n",
    "        review_key = review['_key']\n",
    "        \n",
    "        # Only create edge if product exists\n",
    "        if product_key in product_keys:\n",
    "            edge = {\n",
    "                \"_from\": f\"Products/{product_key}\",\n",
    "                \"_to\": f\"Reviews/{review_key}\"\n",
    "            }\n",
    "            edges.append(edge)\n",
    "    \n",
    "    print(f\"Created {len(edges)} HasReview edges (out of {len(reviews)} reviews)\")\n",
    "    return edges\n",
    "\n",
    "def create_written_by_edges(reviews):\n",
    "    \"\"\"Create edges connecting reviews to users\"\"\"\n",
    "    edges = []\n",
    "    user_keys = set()  # Keep track of valid user keys\n",
    "    \n",
    "    # Get all valid user keys from the database\n",
    "    try:\n",
    "        users_cursor = db.collection(\"Users\").all()\n",
    "        for user in users_cursor:\n",
    "            user_keys.add(user[\"_key\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user keys: {e}\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_key = review['_key']\n",
    "        user_key = review['user_id']\n",
    "        \n",
    "        # Only create edge if user exists\n",
    "        if user_key in user_keys:\n",
    "            edge = {\n",
    "                \"_from\": f\"Reviews/{review_key}\",\n",
    "                \"_to\": f\"Users/{user_key}\"\n",
    "            }\n",
    "            edges.append(edge)\n",
    "    \n",
    "    print(f\"Created {len(edges)} WrittenBy edges (out of {len(reviews)} reviews)\")\n",
    "    return edges\n",
    "\n",
    "def create_belongs_to_category_edges(products):\n",
    "    \"\"\"Create edges connecting products to categories\"\"\"\n",
    "    edges = []\n",
    "    product_keys = set()  # Keep track of valid product keys\n",
    "    category_keys = set()  # Keep track of valid category keys\n",
    "    \n",
    "    # Get all valid product keys from the database\n",
    "    try:\n",
    "        products_cursor = db.collection(\"Products\").all()\n",
    "        for product in products_cursor:\n",
    "            product_keys.add(product[\"_key\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching product keys: {e}\")\n",
    "    \n",
    "    # Get all valid category keys from the database\n",
    "    try:\n",
    "        categories_cursor = db.collection(\"Categories\").all()\n",
    "        for category in categories_cursor:\n",
    "            category_keys.add(category[\"_key\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching category keys: {e}\")\n",
    "    \n",
    "    for product in products:\n",
    "        product_key = product['_key']\n",
    "        if product.get('main_category') and product_key in product_keys:\n",
    "            # Sanitize category key\n",
    "            category_key = sanitize_key(product['main_category'])\n",
    "            \n",
    "            # Only create edge if both product and category exist\n",
    "            if category_key in category_keys:\n",
    "                edge = {\n",
    "                    \"_from\": f\"Products/{product_key}\",\n",
    "                    \"_to\": f\"Categories/{category_key}\"\n",
    "                }\n",
    "                edges.append(edge)\n",
    "    \n",
    "    print(f\"Created {len(edges)} BelongsToCategory edges (out of {len(products)} products)\")\n",
    "    return edges\n",
    "\n",
    "def create_variant_of_edges(products):\n",
    "    \"\"\"Create edges connecting product variants\"\"\"\n",
    "    edges = []\n",
    "    product_keys = set()  # Keep track of valid product keys\n",
    "    \n",
    "    # Get all valid product keys from the database\n",
    "    try:\n",
    "        products_cursor = db.collection(\"Products\").all()\n",
    "        for product in products_cursor:\n",
    "            product_keys.add(product[\"_key\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching product keys: {e}\")\n",
    "    \n",
    "    for product in products:\n",
    "        product_key = product['_key']\n",
    "        parent_key = product.get('parent_asin')\n",
    "        \n",
    "        # Only create edge if both product and parent exist and are different\n",
    "        if (parent_key and parent_key != product_key and \n",
    "            product_key in product_keys and parent_key in product_keys):\n",
    "            edge = {\n",
    "                \"_from\": f\"Products/{product_key}\",\n",
    "                \"_to\": f\"Products/{parent_key}\"\n",
    "            }\n",
    "            edges.append(edge)\n",
    "    \n",
    "    print(f\"Created {len(edges)} VariantOf edges (out of {len(products)} products)\")\n",
    "    return edges\n",
    "\n",
    "# Insert data into ArangoDB collections\n",
    "def insert_documents(collection_name, documents, batch_size=100):\n",
    "    \"\"\"Insert documents into a collection with batch processing\"\"\"\n",
    "    if not documents:\n",
    "        print(f\"No documents to insert into {collection_name}\")\n",
    "        return\n",
    "        \n",
    "    collection = db.collection(collection_name)\n",
    "    \n",
    "    # Process in batches to avoid memory issues with large datasets\n",
    "    successful_inserts = 0\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        try:\n",
    "            result = collection.import_bulk(batch, on_duplicate=\"update\")\n",
    "            successful_inserts += result[\"created\"] + result[\"updated\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting batch into {collection_name}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully inserted/updated {successful_inserts} documents in {collection_name}\")\n",
    "\n",
    "# Transform the data with sanitized keys\n",
    "products = prepare_products_data(metadata_data)\n",
    "reviews = prepare_reviews_data(reviews_data)\n",
    "users = extract_users(reviews)\n",
    "categories = extract_categories(products)\n",
    "\n",
    "# Insert node data\n",
    "print(\"Inserting product data...\")\n",
    "insert_documents(\"Products\", products)\n",
    "\n",
    "print(\"Inserting review data...\")\n",
    "insert_documents(\"Reviews\", reviews)\n",
    "\n",
    "print(\"Inserting user data...\")\n",
    "insert_documents(\"Users\", users)\n",
    "\n",
    "print(\"Inserting category data...\")\n",
    "insert_documents(\"Categories\", categories)\n",
    "\n",
    "# Create edge data (after nodes have been inserted)\n",
    "# This ensures we only create edges between existing nodes\n",
    "has_review_edges = create_has_review_edges(reviews)\n",
    "written_by_edges = create_written_by_edges(reviews)\n",
    "belongs_to_category_edges = create_belongs_to_category_edges(products)\n",
    "variant_of_edges = create_variant_of_edges(products)\n",
    "\n",
    "# Insert edge data\n",
    "print(\"Creating product-review connections...\")\n",
    "insert_documents(\"HasReview\", has_review_edges)\n",
    "\n",
    "print(\"Creating review-user connections...\")\n",
    "insert_documents(\"WrittenBy\", written_by_edges)\n",
    "\n",
    "print(\"Creating product-category connections...\")\n",
    "insert_documents(\"BelongsToCategory\", belongs_to_category_edges)\n",
    "\n",
    "print(\"Creating product variant connections...\")\n",
    "insert_documents(\"VariantOf\", variant_of_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create the NetworkX Integration (for Graph Analytics)\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nxadb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the NetworkX graph from ArangoDB\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m G_adb \u001b[38;5;241m=\u001b[39m \u001b[43mnxadb\u001b[49m\u001b[38;5;241m.\u001b[39mGraph(name\u001b[38;5;241m=\u001b[39mGRAPH_NAME, db\u001b[38;5;241m=\u001b[39mdb)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(G_adb)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nxadb' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the NetworkX graph from ArangoDB\n",
    "G_adb = nxadb.Graph(name=GRAPH_NAME, db=db)\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Generate and Store Embeddings\n",
    "------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-your-openai-api-key\"\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create embeddings for products\n",
    "def generate_product_embeddings():\n",
    "    \"\"\"Generate embeddings for product data and store in ArangoDB\"\"\"\n",
    "    products_collection = db.collection(\"Products\")\n",
    "    batch_size = 25  # Process in small batches to avoid rate limits\n",
    "    \n",
    "    for i in range(0, products_collection.count(), batch_size):\n",
    "        # Get a batch of products\n",
    "        products_batch = list(products_collection.all(limit=batch_size, skip=i))\n",
    "        \n",
    "        # Prepare text for embedding\n",
    "        product_texts = []\n",
    "        for product in products_batch:\n",
    "            # Combine title, features, and description for a rich representation\n",
    "            text = f\"{product.get('title', '')} {product.get('features_text', '')} {product.get('description', '')}\"\n",
    "            product_texts.append(text)\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        print(f\"Generating embeddings for products {i} to {i+len(product_texts)}\")\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(product_texts)\n",
    "            \n",
    "            # Update products with embeddings\n",
    "            for j, product in enumerate(products_batch):\n",
    "                products_collection.update({\n",
    "                    \"_key\": product[\"_key\"],\n",
    "                    \"embedding\": embeddings[j]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "\n",
    "# Create embeddings for reviews\n",
    "def generate_review_embeddings():\n",
    "    \"\"\"Generate embeddings for review data and store in ArangoDB\"\"\"\n",
    "    reviews_collection = db.collection(\"Reviews\")\n",
    "    batch_size = 25  # Process in small batches to avoid rate limits\n",
    "    \n",
    "    for i in range(0, reviews_collection.count(), batch_size):\n",
    "        # Get a batch of reviews\n",
    "        reviews_batch = list(reviews_collection.all(limit=batch_size, skip=i))\n",
    "        \n",
    "        # Prepare text for embedding\n",
    "        review_texts = []\n",
    "        for review in reviews_batch:\n",
    "            # Combine title and text for a rich representation\n",
    "            text = f\"{review.get('title', '')} {review.get('text', '')}\"\n",
    "            review_texts.append(text)\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        print(f\"Generating embeddings for reviews {i} to {i+len(review_texts)}\")\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(review_texts)\n",
    "            \n",
    "            # Update reviews with embeddings\n",
    "            for j, review in enumerate(reviews_batch):\n",
    "                reviews_collection.update({\n",
    "                    \"_key\": review[\"_key\"],\n",
    "                    \"embedding\": embeddings[j]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "\n",
    "# Generate embeddings (uncomment to execute - can take time for large datasets)\n",
    "# generate_product_embeddings()\n",
    "# generate_review_embeddings()\n",
    "print(\"Embeddings generation code is ready but commented out to avoid API costs. Uncomment as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Build the Agentic App with LangChain & LangGraph\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4o\")\n",
    "\n",
    "# Create the ArangoGraph LangChain wrapper\n",
    "arango_graph = ArangoGraph(db)\n",
    "\n",
    "# Define tools for the agent\n",
    "\n",
    "@tool\n",
    "def get_product_by_description(query: str):\n",
    "    \"\"\"\n",
    "    This tool finds products that match a given description.\n",
    "    It searches through product titles, features, and descriptions to find the best matches.\n",
    "    \"\"\"\n",
    "    # Convert query to embedding\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    \n",
    "    # Prepare AQL query with vector search\n",
    "    aql = \"\"\"\n",
    "    FOR product IN Products\n",
    "        LET score = COSINE_SIMILARITY(product.embedding, @embedding)\n",
    "        FILTER score > 0.7\n",
    "        SORT score DESC\n",
    "        LIMIT 5\n",
    "        RETURN {\n",
    "            asin: product._key,\n",
    "            title: product.title,\n",
    "            description: product.description,\n",
    "            price: product.price,\n",
    "            average_rating: product.average_rating,\n",
    "            score: score\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query\n",
    "    cursor = db.aql.execute(aql, bind_vars={\"embedding\": query_embedding})\n",
    "    results = list(cursor)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No products found matching your description.\"\n",
    "    \n",
    "    # Format results\n",
    "    response = \"Here are products that match your description:\\n\\n\"\n",
    "    for i, product in enumerate(results, 1):\n",
    "        response += f\"{i}. {product['title']}\\n\"\n",
    "        response += f\"   Price: ${product['price']}\\n\"\n",
    "        response += f\"   Rating: {product['average_rating']}/5.0\\n\"\n",
    "        response += f\"   ASIN: {product['asin']}\\n\\n\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def get_reviews_for_product(asin: str):\n",
    "    \"\"\"\n",
    "    This tool retrieves reviews for a specific product identified by its ASIN.\n",
    "    It returns the most helpful reviews first.\n",
    "    \"\"\"\n",
    "    # Prepare AQL query\n",
    "    aql = \"\"\"\n",
    "    FOR review IN Reviews\n",
    "        FILTER review.asin == @asin\n",
    "        SORT review.helpful_votes DESC\n",
    "        LIMIT 5\n",
    "        RETURN {\n",
    "            title: review.title,\n",
    "            text: review.text,\n",
    "            rating: review.rating,\n",
    "            helpful_votes: review.helpful_votes,\n",
    "            verified_purchase: review.verified_purchase\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query\n",
    "    cursor = db.aql.execute(aql, bind_vars={\"asin\": asin})\n",
    "    reviews = list(cursor)\n",
    "    \n",
    "    if not reviews:\n",
    "        return f\"No reviews found for product with ASIN {asin}.\"\n",
    "    \n",
    "    # Get product title\n",
    "    product_query = \"\"\"\n",
    "    FOR product IN Products\n",
    "        FILTER product._key == @asin\n",
    "        RETURN product.title\n",
    "    \"\"\"\n",
    "    product_cursor = db.aql.execute(product_query, bind_vars={\"asin\": asin})\n",
    "    product_title = list(product_cursor)[0] if list(product_cursor) else \"Unknown Product\"\n",
    "    \n",
    "    # Format results\n",
    "    response = f\"Reviews for {product_title} (ASIN: {asin}):\\n\\n\"\n",
    "    for i, review in enumerate(reviews, 1):\n",
    "        response += f\"{i}. {review['title']} - {review['rating']}/5.0 stars\\n\"\n",
    "        response += f\"   {review['text']}\\n\"\n",
    "        if review['verified_purchase']:\n",
    "            response += f\"   (Verified Purchase)\\n\"\n",
    "        response += f\"   Helpful votes: {review['helpful_votes']}\\n\\n\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def text_to_aql_to_text(query: str):\n",
    "    \"\"\"\n",
    "    This tool translates a natural language query into AQL, executes it,\n",
    "    and returns the results in natural language.\n",
    "    Use this for complex searches and relationships in the product graph.\n",
    "    \"\"\"\n",
    "    chain = ArangoGraphQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=arango_graph,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    result = chain.invoke(query)\n",
    "    return str(result[\"result\"])\n",
    "\n",
    "@tool\n",
    "def analyze_product_network(query: str):\n",
    "    \"\"\"\n",
    "    This tool uses graph analytics to analyze the product network.\n",
    "    It can identify related products, popular items, and customer patterns.\n",
    "    \"\"\"\n",
    "    # Parse the query to determine what kind of analysis to perform\n",
    "    if \"popular\" in query.lower() or \"best selling\" in query.lower():\n",
    "        # Find most reviewed products\n",
    "        aql = \"\"\"\n",
    "        FOR product IN Products\n",
    "            SORT product.rating_count DESC\n",
    "            LIMIT 5\n",
    "            RETURN {\n",
    "                asin: product._key,\n",
    "                title: product.title,\n",
    "                reviews: product.rating_count,\n",
    "                rating: product.average_rating\n",
    "            }\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        products = list(cursor)\n",
    "        \n",
    "        response = \"Most popular products based on number of reviews:\\n\\n\"\n",
    "        for i, product in enumerate(products, 1):\n",
    "            response += f\"{i}. {product['title']}\\n\"\n",
    "            response += f\"   Total reviews: {product['reviews']}\\n\"\n",
    "            response += f\"   Average rating: {product['rating']}/5.0\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    elif \"similar\" in query.lower() or \"related\" in query.lower():\n",
    "        # Extract ASIN from query if present\n",
    "        asin_match = re.search(r'[A-Z0-9]{10}', query)\n",
    "        if asin_match:\n",
    "            asin = asin_match.group(0)\n",
    "            \n",
    "            # Find related products (variants or commonly bought together)\n",
    "            aql = \"\"\"\n",
    "            LET variants = (\n",
    "                FOR v, e IN 1..1 ANY @asin VariantOf\n",
    "                    RETURN v\n",
    "            )\n",
    "            \n",
    "            LET product = DOCUMENT(CONCAT('Products/', @asin))\n",
    "            \n",
    "            LET similar_products = (\n",
    "                FOR other IN Products\n",
    "                    FILTER other._key != @asin\n",
    "                    FILTER other.main_category == product.main_category\n",
    "                    SORT ABS(other.price - product.price) ASC\n",
    "                    LIMIT 3\n",
    "                    RETURN other\n",
    "            )\n",
    "            \n",
    "            RETURN {\n",
    "                variants: variants,\n",
    "                similar: similar_products,\n",
    "                original: product\n",
    "            }\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor = db.aql.execute(aql, bind_vars={\"asin\": asin})\n",
    "            result = next(cursor, None)\n",
    "            \n",
    "            if not result or not result.get('original'):\n",
    "                return f\"Could not find product with ASIN {asin}.\"\n",
    "            \n",
    "            response = f\"Analysis for product: {result['original'].get('title', 'Unknown')}\\n\\n\"\n",
    "            \n",
    "            if result['variants']:\n",
    "                response += \"Product variants:\\n\"\n",
    "                for i, variant in enumerate(result['variants'], 1):\n",
    "                    response += f\"{i}. {variant.get('title', 'Unknown')}\\n\"\n",
    "                    response += f\"   Price: ${variant.get('price', 0)}\\n\"\n",
    "                    response += f\"   ASIN: {variant.get('_key', 'Unknown')}\\n\\n\"\n",
    "            \n",
    "            if result['similar']:\n",
    "                response += \"Similar products by price and category:\\n\"\n",
    "                for i, similar in enumerate(result['similar'], 1):\n",
    "                    response += f\"{i}. {similar.get('title', 'Unknown')}\\n\"\n",
    "                    response += f\"   Price: ${similar.get('price', 0)}\\n\"\n",
    "                    response += f\"   ASIN: {similar.get('_key', 'Unknown')}\\n\\n\"\n",
    "            \n",
    "            return response\n",
    "        else:\n",
    "            return \"To find similar products, please provide a valid ASIN (10-character Amazon product ID).\"\n",
    "    \n",
    "    else:\n",
    "        # Default to general graph statistics\n",
    "        product_count = db.collection(\"Products\").count()\n",
    "        review_count = db.collection(\"Reviews\").count()\n",
    "        user_count = db.collection(\"Users\").count()\n",
    "        \n",
    "        return f\"\"\"\n",
    "        Graph Analytics Summary:\n",
    "        \n",
    "        - Total Products: {product_count}\n",
    "        - Total Reviews: {review_count}\n",
    "        - Total Users: {user_count}\n",
    "        \n",
    "        To get more specific analytics, try asking about:\n",
    "        - Popular or best-selling products\n",
    "        - Similar or related products (with an ASIN)\n",
    "        - Category trends\n",
    "        \"\"\"\n",
    "\n",
    "# Create the Agent\n",
    "def create_graph_rag_agent():\n",
    "    \"\"\"Create and return the GraphRAG agent\"\"\"\n",
    "    tools = [\n",
    "        get_product_by_description,\n",
    "        get_reviews_for_product,\n",
    "        text_to_aql_to_text,\n",
    "        analyze_product_network\n",
    "    ]\n",
    "    \n",
    "    return create_react_agent(llm, tools)\n",
    "\n",
    "# Function to query the agent\n",
    "def query_graph_rag(query):\n",
    "    \"\"\"Query the GraphRAG agent with a user question\"\"\"\n",
    "    agent = create_graph_rag_agent()\n",
    "    final_state = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "    return final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Testing the GraphRAG System\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find products similar to moisturizer\",\n",
    "    \"What are the reviews for product B088SZDGXG?\",\n",
    "    \"What's the most popular beauty product?\",\n",
    "    \"Tell me about products with the highest ratings\",\n",
    "    \"Find products under $10 with good reviews\"\n",
    "]\n",
    "\n",
    "# Uncomment to test the agent\n",
    "# for query in test_queries:\n",
    "#     print(f\"\\nQuery: {query}\")\n",
    "#     print(\"-\" * 50)\n",
    "#     response = query_graph_rag(query)\n",
    "#     print(response)\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "print(\"GraphRAG implementation complete! Run the test queries to see it in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Process Multiple Categories\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_categories(download_links, limit=None, batch_size=1000):\n",
    "    \"\"\"Process multiple categories with rate limiting and batch processing\"\"\"\n",
    "    categories_processed = 0\n",
    "    \n",
    "    # Create a processing queue from the download links\n",
    "    categories_to_process = list(download_links.keys())\n",
    "    if limit:\n",
    "        categories_to_process = categories_to_process[:limit]\n",
    "    \n",
    "    print(f\"Will process {len(categories_to_process)} categories\")\n",
    "    \n",
    "    for category in categories_to_process:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing category {categories_processed + 1}/{len(categories_to_process)}: {category}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get the download URL for this category\n",
    "        review_url = download_links[category]\n",
    "        \n",
    "        # Process the category data\n",
    "        reviews_data, metadata_data = process_category(category, review_url)\n",
    "        \n",
    "        # Skip if no data was loaded\n",
    "        if not reviews_data:\n",
    "            print(f\"No review data available for {category}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Transform the data\n",
    "        print(f\"Transforming data for {category}...\")\n",
    "        category_products = prepare_products_data(metadata_data)\n",
    "        category_reviews = prepare_reviews_data(reviews_data)\n",
    "        category_users = extract_users(category_reviews)\n",
    "        category_categories = extract_categories(category_products)\n",
    "        \n",
    "        # Create edge data\n",
    "        category_has_review_edges = create_has_review_edges(category_reviews)\n",
    "        category_written_by_edges = create_written_by_edges(category_reviews)\n",
    "        category_belongs_to_category_edges = create_belongs_to_category_edges(category_products)\n",
    "        category_variant_of_edges = create_variant_of_edges(category_products)\n",
    "        \n",
    "        # Insert data in batches\n",
    "        print(f\"Inserting data for {category} into ArangoDB...\")\n",
    "        \n",
    "        # Insert node data\n",
    "        print(\"Inserting product data...\")\n",
    "        insert_documents(\"Products\", category_products, batch_size)\n",
    "        \n",
    "        print(\"Inserting review data...\")\n",
    "        insert_documents(\"Reviews\", category_reviews, batch_size)\n",
    "        \n",
    "        print(\"Inserting user data...\")\n",
    "        insert_documents(\"Users\", category_users, batch_size)\n",
    "        \n",
    "        print(\"Inserting category data...\")\n",
    "        insert_documents(\"Categories\", category_categories, batch_size)\n",
    "        \n",
    "        # Insert edge data\n",
    "        print(\"Creating product-review connections...\")\n",
    "        insert_documents(\"HasReview\", category_has_review_edges, batch_size)\n",
    "        \n",
    "        print(\"Creating review-user connections...\")\n",
    "        insert_documents(\"WrittenBy\", category_written_by_edges, batch_size)\n",
    "        \n",
    "        print(\"Creating product-category connections...\")\n",
    "        insert_documents(\"BelongsToCategory\", category_belongs_to_category_edges, batch_size)\n",
    "        \n",
    "        print(\"Creating product variant connections...\")\n",
    "        insert_documents(\"VariantOf\", category_variant_of_edges, batch_size)\n",
    "        \n",
    "        # Generate embeddings for this category\n",
    "        if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            print(f\"Generating embeddings for {category} products and reviews...\")\n",
    "            generate_category_embeddings(category, batch_size=25)\n",
    "        \n",
    "        # Increment the counter\n",
    "        categories_processed += 1\n",
    "        \n",
    "        # Add a delay between categories to avoid overwhelming the database\n",
    "        if categories_processed < len(categories_to_process):\n",
    "            delay = 5  # seconds\n",
    "            print(f\"Waiting {delay} seconds before processing the next category...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(f\"\\nProcessed {categories_processed}/{len(categories_to_process)} categories successfully\")\n",
    "\n",
    "def generate_category_embeddings(category, batch_size=25):\n",
    "    \"\"\"Generate embeddings for products and reviews of a specific category\"\"\"\n",
    "    # Query products in this category\n",
    "    aql_products = \"\"\"\n",
    "    FOR p IN Products\n",
    "        FILTER p.main_category == @category AND NOT HAS(p, \"embedding\")\n",
    "        LIMIT @batch_size\n",
    "        RETURN p\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process products in batches\n",
    "    while True:\n",
    "        products_batch = list(db.aql.execute(aql_products, bind_vars={\"category\": category, \"batch_size\": batch_size}))\n",
    "        if not products_batch:\n",
    "            break\n",
    "            \n",
    "        # Prepare text for embedding\n",
    "        product_texts = []\n",
    "        product_keys = []\n",
    "        for product in products_batch:\n",
    "            # Combine title, features, and description\n",
    "            text = f\"{product.get('title', '')} {product.get('features_text', '')} {product.get('description', '')}\"\n",
    "            product_texts.append(text)\n",
    "            product_keys.append(product[\"_key\"])\n",
    "        \n",
    "        # Generate embeddings\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(product_texts)\n",
    "            \n",
    "            # Update products with embeddings\n",
    "            for i, key in enumerate(product_keys):\n",
    "                db.collection(\"Products\").update({\n",
    "                    \"_key\": key,\n",
    "                    \"embedding\": embeddings[i]\n",
    "                })\n",
    "            print(f\"Added embeddings to {len(product_keys)} products\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating product embeddings: {e}\")\n",
    "    \n",
    "    # Query reviews for products in this category\n",
    "    aql_reviews = \"\"\"\n",
    "    FOR r IN Reviews\n",
    "        FILTER EXISTS(FOR p IN Products FILTER p._key == r.asin AND p.main_category == @category RETURN 1)\n",
    "        AND NOT HAS(r, \"embedding\")\n",
    "        LIMIT @batch_size\n",
    "        RETURN r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process reviews in batches\n",
    "    while True:\n",
    "        reviews_batch = list(db.aql.execute(aql_reviews, bind_vars={\"category\": category, \"batch_size\": batch_size}))\n",
    "        if not reviews_batch:\n",
    "            break\n",
    "            \n",
    "        # Prepare text for embedding\n",
    "        review_texts = []\n",
    "        review_keys = []\n",
    "        for review in reviews_batch:\n",
    "            text = f\"{review.get('title', '')} {review.get('text', '')}\"\n",
    "            review_texts.append(text)\n",
    "            review_keys.append(review[\"_key\"])\n",
    "        \n",
    "        # Generate embeddings\n",
    "        try:\n",
    "            embeddings = embeddings_model.embed_documents(review_texts)\n",
    "            \n",
    "            # Update reviews with embeddings\n",
    "            for i, key in enumerate(review_keys):\n",
    "                db.collection(\"Reviews\").update({\n",
    "                    \"_key\": key,\n",
    "                    \"embedding\": embeddings[i]\n",
    "                })\n",
    "            print(f\"Added embeddings to {len(review_keys)} reviews\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating review embeddings: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Visualize a subset of the graph\n",
    "def visualize_graph_sample():\n",
    "    \"\"\"Visualize a small sample of the graph\"\"\"\n",
    "    # Get a sample of products\n",
    "    products = list(db.collection(\"Products\").all().limit(5))\n",
    "    product_asins = [p[\"_key\"] for p in products]\n",
    "    \n",
    "    # Create a small NetworkX graph for visualization\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add product nodes\n",
    "    for product in products:\n",
    "        G.add_node(product[\"_key\"], type=\"product\", label=product[\"title\"][:20])\n",
    "    \n",
    "    # Get reviews for these products\n",
    "    for asin in product_asins:\n",
    "        reviews = db.aql.execute(\n",
    "            \"FOR r IN Reviews FILTER r.asin == @asin LIMIT 3 RETURN r\",\n",
    "            bind_vars={\"asin\": asin}\n",
    "        )\n",
    "        \n",
    "        # Add review nodes and edges\n",
    "        for review in reviews:\n",
    "            G.add_node(review[\"_key\"], type=\"review\", label=\"Review\")\n",
    "            G.add_edge(asin, review[\"_key\"], type=\"has_review\")\n",
    "            \n",
    "            # Add user node and edge\n",
    "            user_id = review[\"user_id\"]\n",
    "            G.add_node(user_id, type=\"user\", label=f\"User\")\n",
    "            G.add_edge(review[\"_key\"], user_id, type=\"written_by\")\n",
    "    \n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw nodes with different colors by type\n",
    "    node_colors = {\n",
    "        \"product\": \"skyblue\",\n",
    "        \"review\": \"lightgreen\",\n",
    "        \"user\": \"salmon\"\n",
    "    }\n",
    "    \n",
    "    for node_type, color in node_colors.items():\n",
    "        nodes = [n for n, data in G.nodes(data=True) if data.get(\"type\") == node_type]\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=color, node_size=300)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {n: data.get(\"label\", n) for n, data in G.nodes(data=True)}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title(\"Sample of Amazon Reviews Graph\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Process all categories (comment this out for testing or to process just a subset)\n",
    "# process_all_categories(download_links, limit=5)  # Process just the first 5 categories\n",
    "\n",
    "# To process just one category for testing\n",
    "# process_all_categories({CATEGORY: download_links[CATEGORY]})\n",
    "\n",
    "# Uncomment to visualize a sample of the graph\n",
    "# visualize_graph_sample()\n",
    "\n",
    "print(\"Script is ready to process all Amazon product categories.\")\n",
    "print(\"To run the full processing, uncomment the process_all_categories() line.\")\n",
    "print(\"You can set a limit to process only a subset of categories for testing.\")\n",
    "print(\"Make sure to set your OpenAI API key for embedding generation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
